{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnpa0fYD9QGQDj9QTw+pAU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpaj12/LEARNING_GNNs/blob/main/GNN_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing from: https://www.youtube.com/watch?v=JtDgmmQ60x8&ab_channel=AntonioLonga"
      ],
      "metadata": {
        "id": "wQBrj0AbYKFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to start up Torch - https://github.com/pyg-team/pytorch_geometric"
      ],
      "metadata": {
        "id": "ZTRFJfAcY8rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial7\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaipk3oMY36U",
        "outputId": "b78053e4-1f2b-417d-8e9f-6048a81ee338"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-bb5212b8e35a>:12: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('svg', 'pdf') # For export\n",
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More torch setup - https://github.com/pyg-team/pytorch_geometric"
      ],
      "metadata": {
        "id": "XuJ0f5-VZEfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch geometric\n",
        "try:\n",
        "    import torch_geometric\n",
        "except ModuleNotFoundError:\n",
        "    # Installing torch geometric packages with specific CUDA+PyTorch version.\n",
        "    # See https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for details\n",
        "    TORCH = torch.__version__.split('+')[0]\n",
        "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "\n",
        "    !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-geometric\n",
        "    import torch_geometric\n",
        "import torch_geometric.nn as geom_nn\n",
        "import torch_geometric.data as geom_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lFvgij8YrKF",
        "outputId": "042045f9-d33e-4977-e223-a36dd77d6520"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt26cu124\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt26cu124\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (2.0.2)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt26cu124\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.6.0+cu124.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt26cu124\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing - minute 27 - https://www.youtube.com/watch?v=JtDgmmQ60x8&ab_channel=AntonioLonga"
      ],
      "metadata": {
        "id": "GM7J7j8PZaoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dJxwrmH8YDiK"
      },
      "outputs": [],
      "source": [
        "import torch_geometric\n",
        "from torch_geometric.datasets import Planetoid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Planetoid(root=\"tutorial1\", name=\"Cora\")"
      ],
      "metadata": {
        "id": "8-dxHfztZn9z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KA193LavZ0i9",
        "outputId": "b2f57bbe-726d-4863-fa58-ca338afdd753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cora()\n",
            "number of graphs:\t\t 1\n",
            "number of classes:\t\t 7\n",
            "number of node features:\t 1433\n",
            "number of edge features:\t 0\n"
          ]
        }
      ],
      "source": [
        "print(dataset)\n",
        "print(\"number of graphs:\\t\\t\",len(dataset))\n",
        "print(\"number of classes:\\t\\t\",dataset.num_classes)\n",
        "print(\"number of node features:\\t\",dataset.num_node_features)\n",
        "print(\"number of edge features:\\t\",dataset.num_edge_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k_-e4BYbZ0i9",
        "outputId": "0a40c1d8-6b37-417b-d740-f7ebc13b6607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
          ]
        }
      ],
      "source": [
        "print(dataset.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jFP9cWv1Z0i9",
        "outputId": "b9a0088e-45a3-469c-9d2e-4cc9c09e7fa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "edge_index:\t\t torch.Size([2, 10556])\n",
            "tensor([[ 633, 1862, 2582,  ...,  598, 1473, 2706],\n",
            "        [   0,    0,    0,  ..., 2707, 2707, 2707]])\n",
            "\n",
            "\n",
            "train_mask:\t\t torch.Size([2708])\n",
            "tensor([ True,  True,  True,  ..., False, False, False])\n",
            "\n",
            "\n",
            "x:\t\t torch.Size([2708, 1433])\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "\n",
            "\n",
            "y:\t\t torch.Size([2708])\n",
            "tensor([3, 4, 4,  ..., 3, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "print(\"edge_index:\\t\\t\",dataset.data.edge_index.shape)\n",
        "print(dataset.data.edge_index)\n",
        "print(\"\\n\")\n",
        "print(\"train_mask:\\t\\t\",dataset.data.train_mask.shape)\n",
        "print(dataset.data.train_mask)\n",
        "print(\"\\n\")\n",
        "print(\"x:\\t\\t\",dataset.data.x.shape)\n",
        "print(dataset.data.x)\n",
        "print(\"\\n\")\n",
        "print(\"y:\\t\\t\",dataset.data.y.shape)\n",
        "print(dataset.data.y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "N5iFTpuGZ0i-"
      },
      "outputs": [],
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4Vqq9GAOZ0i-"
      },
      "outputs": [],
      "source": [
        "data = dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nAXAD-lCZ0i-"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv = SAGEConv(dataset.num_features,\n",
        "                             dataset.num_classes,\n",
        "                             aggr=\"max\") # max, mean, add ...)\n",
        "\n",
        "    def forward(self):\n",
        "        x = self.conv(data.x, data.edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "scrolled": true,
        "id": "YKnTRiPGZ0i-"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() and use_cuda_if_available else 'cpu')\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qGndH3q3Z0i-",
        "outputId": "2843ea6d-770c-45c1-b123-cd84a92a70cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4jZkr_DMZ0i-"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    logits, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ixXW8qdxZ0i-",
        "outputId": "1125f7fb-ce77-4d44-b248-7e5846ad9b3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 010, Val: 0.7260, Test: 0.7180\n",
            "Epoch: 020, Val: 0.7260, Test: 0.7180\n",
            "Epoch: 030, Val: 0.7260, Test: 0.7180\n",
            "Epoch: 040, Val: 0.7260, Test: 0.7180\n",
            "Epoch: 050, Val: 0.7260, Test: 0.7180\n",
            "Epoch: 060, Val: 0.7260, Test: 0.7180\n",
            "Epoch: 070, Val: 0.7260, Test: 0.7180\n",
            "Epoch: 080, Val: 0.7280, Test: 0.7100\n",
            "Epoch: 090, Val: 0.7340, Test: 0.7170\n"
          ]
        }
      ],
      "source": [
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1,100):\n",
        "    train()\n",
        "    _, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    log = 'Epoch: {:03d}, Val: {:.4f}, Test: {:.4f}'\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(log.format(epoch, best_val_acc, test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Started Tutorial 3 - GAT implementation**\n"
      ],
      "metadata": {
        "id": "2hRUNQrbdwD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q98sFN0ieKv-"
      },
      "outputs": [],
      "source": [
        "class GATLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple PyTorch Implementation of the Graph Attention layer.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(GATLayer, self).__init__()\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1VIXSKaVeKv_",
        "outputId": "a7fa4aed-3681-4176-e6a9-5a973cf20417",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2])\n"
          ]
        }
      ],
      "source": [
        "in_features = 5\n",
        "out_features = 2\n",
        "nb_nodes = 3\n",
        "\n",
        "W = nn.Parameter(torch.zeros(size=(in_features, out_features))) #xavier paramiter inizializator\n",
        "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
        "\n",
        "input = torch.rand(nb_nodes,in_features)\n",
        "\n",
        "\n",
        "# linear transformation\n",
        "h = torch.mm(input, W)\n",
        "N = h.size()[0]\n",
        "\n",
        "print(h.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tEsgB7YWeKwA",
        "outputId": "78f1cbe9-329e-4e3e-f4fb-5e35d97a52bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1])\n"
          ]
        }
      ],
      "source": [
        "a = nn.Parameter(torch.zeros(size=(2*out_features, 1))) #xavier paramiter inizializator\n",
        "nn.init.xavier_uniform_(a.data, gain=1.414)\n",
        "print(a.shape)\n",
        "\n",
        "leakyrelu = nn.LeakyReLU(0.2)  # LeakyReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qwuChJBaeKwA"
      },
      "outputs": [],
      "source": [
        "a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * out_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mWRMB90ieKwA"
      },
      "outputs": [],
      "source": [
        "e = leakyrelu(torch.matmul(a_input, a).squeeze(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SxOvdOlZeKwD",
        "outputId": "24539326-99e2-4f43-cb2b-58441408beb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3, 4]) torch.Size([4, 1])\n",
            "\n",
            "torch.Size([3, 3, 1])\n",
            "\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "print(a_input.shape,a.shape)\n",
        "print(\"\")\n",
        "print(torch.matmul(a_input,a).shape)\n",
        "print(\"\")\n",
        "print(torch.matmul(a_input,a).squeeze(2).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wM5aflD1eKwD",
        "outputId": "d48d7535-46b7-4eb3-ca1f-80caa6293d76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "# Masked Attention\n",
        "adj = torch.randint(2, (3, 3))\n",
        "\n",
        "zero_vec  = -9e15*torch.ones_like(e)\n",
        "print(zero_vec.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zdfIEjNdeKwE",
        "outputId": "55a6fa0d-7600-4871-e1a9-8a3337939b77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 0, 0],\n",
            "        [1, 1, 1],\n",
            "        [0, 1, 0]]) \n",
            " tensor([[5.0360, 4.2603, 4.0851],\n",
            "        [4.3929, 3.6172, 3.4420],\n",
            "        [4.2159, 3.4401, 3.2649]], grad_fn=<LeakyReluBackward0>) \n",
            " tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
            "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
            "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5.0360e+00, -9.0000e+15, -9.0000e+15],\n",
              "        [ 4.3929e+00,  3.6172e+00,  3.4420e+00],\n",
              "        [-9.0000e+15,  3.4401e+00, -9.0000e+15]], grad_fn=<WhereBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "attention = torch.where(adj > 0, e, zero_vec)\n",
        "print(adj,\"\\n\",e,\"\\n\",zero_vec)\n",
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YMKiTxHOeKwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0c3af0-c5e8-4b8e-96a8-c9f555acb190"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5761, 0.2119, 0.2119],\n",
              "        [0.4059, 0.3030, 0.2911],\n",
              "        [0.2119, 0.5761, 0.2119]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "attention = F.softmax(attention, dim=1)\n",
        "h_prime   = torch.matmul(attention, h)\n",
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZheW4XSIeKwE",
        "outputId": "a819c2cc-0874-4acb-ee7f-e01d3117be76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8778,  1.2413],\n",
              "        [-0.7993,  1.1988],\n",
              "        [-0.7891,  1.0725]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "h_prime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1O7VK5H6eKwE",
        "outputId": "ba93a226-c829-4c4e-f2c4-e6f3eca02846",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.8778,  1.2413],\n",
            "        [-0.7993,  1.1988],\n",
            "        [-0.7891,  1.0725]], grad_fn=<MmBackward0>) \n",
            " tensor([[-1.0801,  1.3405],\n",
            "        [-0.8366,  0.8770],\n",
            "        [-0.3690,  1.3362]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(h_prime,\"\\n\",h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pxYCmfSueKwF"
      },
      "outputs": [],
      "source": [
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GATLayer, self).__init__()\n",
        "\n",
        "        '''\n",
        "        TODO\n",
        "        '''\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # Linear Transformation\n",
        "        h = torch.mm(input, self.W) # matrix multiplication\n",
        "        N = h.size()[0]\n",
        "\n",
        "        # Attention Mechanism\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e       = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "\n",
        "        # Masked Attention\n",
        "        zero_vec  = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime   = torch.matmul(attention, h)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PTxehnsaeKwF"
      },
      "outputs": [],
      "source": [
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.dropout       = dropout        # drop prob = 0.6\n",
        "        self.in_features   = in_features    #\n",
        "        self.out_features  = out_features   #\n",
        "        self.alpha         = alpha          # LeakyReLU with negative input slope, alpha = 0.2\n",
        "        self.concat        = concat         # conacat = True for all layers except the output layer.\n",
        "\n",
        "\n",
        "        # Xavier Initialization of Weights\n",
        "        # Alternatively use weights_init to apply weights of choice\n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "\n",
        "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        # LeakyReLU\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # Linear Transformation\n",
        "        h = torch.mm(input, self.W) # matrix multiplication\n",
        "        N = h.size()[0]\n",
        "        print(N)\n",
        "\n",
        "        # Attention Mechanism\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e       = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "\n",
        "        # Masked Attention\n",
        "        zero_vec  = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime   = torch.matmul(attention, h)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the stuff below is how the calculations are put together...next is using GATConv which is the torches library multihead implementation\n"
      ],
      "metadata": {
        "id": "N4S2q5iNmqCB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "v9MXM2deeKwF",
        "outputId": "048d891f-54e4-4edd-949b-78923814371e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Classes in Cora: 7\n",
            "Number of Node Features in Cora: 1433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "name_data = 'Cora'\n",
        "dataset = Planetoid(root= '/tmp/' + name_data, name = name_data)\n",
        "dataset.transform = T.NormalizeFeatures()\n",
        "\n",
        "print(f\"Number of Classes in {name_data}:\", dataset.num_classes)\n",
        "print(f\"Number of Node Features in {name_data}:\", dataset.num_node_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "C2o83bImeKwG",
        "outputId": "b060672d-0895-4ca1-ccea-def2f05a9bdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "tensor(1.9481, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6679, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6086, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5864, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5363, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GAT, self).__init__()\n",
        "        self.hid = 8\n",
        "        self.in_head = 8\n",
        "        self.out_head = 1\n",
        "\n",
        "\n",
        "        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
        "        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False,\n",
        "                             heads=self.out_head, dropout=0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "''''device = \"cpu\"'''\n",
        "\n",
        "model = GAT().to(device)\n",
        "data = dataset[0].to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1000):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "    if epoch%200 == 0:\n",
        "        print(loss)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hNSpoecIeKwG",
        "outputId": "e335914c-a1c2-4a83-ec05-d3549731c4a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8220\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "_, pred = model(data).max(dim=1)\n",
        "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "acc = correct / data.test_mask.sum().item()\n",
        "print('Accuracy: {:.4f}'.format(acc))"
      ]
    }
  ]
}